# -*- coding: utf-8 -*-
"""Week2_Template_NR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JHbIfKN5lTgvibrMxcIPqy7rv9py4O76

# Nonlinear  Regression

Fill the blank spaces as required.

Do not change name of any class, method name.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from matplotlib import pyplot as plt

# %matplotlib inline


class nlr:
  # Evaluates the gradient of cost function (J). Hint: You can use this to optimize w
  def grad(self, x, y, w):
    n = y.size
    grad_J = (1/n)*np.dot((np.dot(x, w) - y), x)
    return grad_J

  # This function calculates the cost (J)
  def computeCost(self, x, y, w):
    J = 0    # J is cost function
    m = y.size
    J = (1/(2*m))*np.sum(np.square(np.matmul(x, w) - y))
    # write your code to calculate J
    return J

  # This function optimizes the weights w_0, w_1, w_2. Batch Gradient Descent method
  def BgradientDescent(self, x, y, w, alpha, iters):
    m = y.size   # number of training examples
    w = w.copy()  # To keep a copy of original weights

    J_history = []   # Use a python list to save cost in every iteration

    for i in range(iters):
      w = w - (alpha) * self.grad(x, y, w)
      # Loop to update weights (w vector)
      # Also save cost at every step
      J_history.append(self.computeCost(x, y, w))

    return w, J_history

  # This function optimizes the weights w_0, w_1, w_2. Stochastic Gradient Descent method
  def SgradientDescent(self, x, y, w, alpha, iters):
    m = y.size()   # number of training examples
    w = w.copy()  # To keep a copy of original weights

    J_history_s = []   # Use a python list to save cost in every iteration

    for i in range(iters):
      r = np.random.randint() # generate random integers (refer lab demo code)
      # create randomly a minibatch from whole data set and find weights based on that new data set.
      # Loop to update weights (w vector)
      # Also save cost at every step
      
      err = (np.dot((np.dot(x[r:r+10], w) - y[r:r+10]), x[r:r+10]))
      w = w - (alpha/m) * err
      
      J_history_s.append(self.computeCost(x, y, w))

    return w, J_history_s

  # This function implements line search Secant method
  # refer to class notes on optimization and lab demo copy.
  def ls_secant(self, x, y, w, d):
    # d is search direction d = -grad(J). Refer class and Lab notes
    epsilon = 10**(-4)  # Line search tolerance

    alpha_curr = 0     # Alpha (x_i-1)
    alpha = 0.01       # initial value (x_i)

    # dphi_zero = (d^T)(grad J(w_0) # At every alpha updation loop you will have a given initial weight vector (w_0)
    dphi_zero = np.dot(d, self.grad(x, y, w))
    dphi_curr = dphi_zero  # required for first alpha iteration
    i = 0
    while abs(dphi_curr) > (epsilon*abs(dphi_zero)):  # tolerance or looping criteria used here
      # write loop to update alpha
      alpha_old = alpha_curr
      alpha_curr = alpha
      
      dphi_old = dphi_curr
      w1 = w - alpha_curr * d
      
      dphi_curr = np.dot(d, self.grad(x, y, w1))
      alpha = (dphi_curr*alpha_old - dphi_old*alpha_curr)/(dphi_curr - dphi_old)
      
      i = i + 1
    return alpha

  # This function optimizes the weights w_0, w_1, w_2. Batch Gradient Descent method using variable alpha which you previously updated using ls_secant() method
  def AgradientDescent(self, x, y, w, iters):
    m = y.size   # number of training examples
    w = w.copy()  # To keep a copy of original weights
    eps = 10**(-12)  # tolerance for J_history

    J_history_a = [0]   # Use a python list to save cost in every iteration

    for i in range(iters):
      d = (-1)*self.grad(x, y, w) # d is search direction d = -grad(J)
      alpha = self.ls_secant(x, y, w, d)  # update alpha at every iteration
      
      w = w - (alpha)*d
      J_history_a.append(self.computeCost(x, y, w))
      # Loop to update weights (w vector)
      # Also save cost at every step in J_history_a
      # stopping criteria
      if (J_history_a[i+1] - J_history_a[i]) < eps:
        print('No. of iterations', i)
        break

    return w, J_history_a

if __name__ == "__main__":
    input_data = []
    file = open("prob1data.txt", "r")
    file = file.readlines()
    print(file[1].split(","))
    
    for i in range(len(file)):
        items = file[i].split(",")
        listitems = []
        for item in items:
            listitems.append(float(item))
        input_data.append(listitems)
        print(input_data)
    for i in range(len(input_data)):
        plt.figure()
        plt.plot(input_data[i])
        plt.show()
